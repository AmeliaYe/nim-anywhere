One of the primary benefit for AI for Enterprises is their ability to work with and learn from their internal data. [Retrieval-Augmented Generation](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/) (RAG) is one of the best way to do so. Nvidia has developed a set of micro-services called [NIM](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html#:~:text=NVIDIA%20NIM%20is%20a%20set,to%20inference%20with%20unparalleled%20performance)(NVIDIA Inference Micro-service) to help our partners and customers build effective RAG pipeline with ease. 

NIM Anywhere is an integration of all the tooling required to start integrating NIMs. It natively scales out to full-sized labs and up to production environments. This is great news for building a RAG architecture and easily adding NIMs as needed! If you're unfamiliar with RAG(Retrieval Augmented Generation), it is an architecture that combines the AI model with a retrieval system, allowing models to pull relevant external information. Imagine you're the [xx] of a company with a local database containing confidential, up-to-date information. You donâ€™t want OpenAI to access it, but you need the model to understand it to answer questions accurately. The solution? Connect your language model to the database and feed them with the information. 

To learn more about why it's an excellent solution for boosting the accuracy and reliability of your generative AI models, [click me](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)!

Get started with NIM Anywhere now with the [quick start](#quick-start) instructions and build your first RAG application!

![NIM Anywhere Screenshot](_static/nim-anywhere.png)
